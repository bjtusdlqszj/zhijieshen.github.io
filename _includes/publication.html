<head>
	<style>
		body{
			color: #333;
		}
		#container{
			min-width: 1000px;
			width: 1000px;
/*			overflow: auto;
*/			margin: 50px auto;padding: 30px;
			/*zoom: 1;*/
*//*			border: 1px solid #ccc;background: #fc9;color: #fff;
*/		}
		#left{
			float: left;
			width: 400px;
			height: 230px;
			margin-left: 0px;
		}
		#right{
			float: left;
			width: auto;
			margin-left: 50px;
		}
		#name{
			font-size: 22.0pt;
		    mso-bidi-font-size: 24.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		        font-weight: bold;
		}
		#info{
		    font-size: 16.0pt;
		    mso-bidi-font-size: 17.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		    margin-top: 30px;
		    margin-left: 5px;
		    margin-bottom: 10px;
		    
		}
		.clear{clear:both; height: 0; line-height: 0; font-size: 0}
		.Bio{
			font-size:16.0pt;
			mso-bidi-font-size:17.0pt;
			line-height:150%;
			font-family:Times;
			mso-bidi-font-family:Lato-Regular;
			text-align: justify;
		}
		span.SpellE {
		    mso-style-name: "";
		    mso-spl-e: yes;
		}
		span.Title{
			    font-size: 22.0pt;
			    mso-bidi-font-size: 17.0pt;
			    font-family: Times;
			    mso-bidi-font-family: Lato-Regular;
			    font: bold;
			    margin-top: 10px;
		}
		div.section{
			padding-top: 30px;
		}
		
		div.sub-left{
			float: left;
			width: 250px;
						
		}
		div.sub-left img{
			vertical-align: middle;
			horizontal-align: middle;
			margin-top: 10px;
		}
		
		div.sub-left span{
			height: 100%;
			display: inline-block;
			vertical-align: top;
						
		}
		div.sub-right{
			float: left;
			width: 700px;			
		}
		.paper{
			overflow: auto;
			zoom:1;
			padding-bottom: 0px;
			min-height: 150px;

		}
		.paperTitle{
			font-size:14.0pt;
			mso-bidi-font-size:18.0pt;
			font-family:Times;
			mso-bidi-font-family:Times;
			margin-top: 10px;
			margin-bottom: 10px;
			font-weight: bold;
		}
		.paperName,.paperPub{
		    font-size: 12.0pt;
		    mso-bidi-font-size: 13.0pt;		    
		    font-family: Times;
		    mso-bidi-font-family: Times;
		    line-height:150%;
		}
		.link{
		    font-size: 12.0pt;
		    mso-bidi-font-size: 13.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		    margin-top: 10px;
		    margin-bottom: 0px;
		}
		.special{
		    margin-top: 0in;
		    margin-bottom: 0in;
		    margin-left: -.9pt;
		    margin-bottom: .0001pt;
		    text-indent: .9pt;
		    mso-pagination: none;
		    tab-stops: 13.75in;
		    mso-layout-grid-align: none;
		    text-autospace: none;
		}
		.long div.sub-left, .long div.sub-right{
			height: 300px;
			width: 950px;

		}
		.short div.sub-left, .short div.sub-right{
			height:160px;

		}
		div.sub-left,div.sub-right{
			height:200px;

		}
	</style>
</head>


<h3>
	<a name='publications'></a> Publications
</h3>

<h4>
	<a name='pre'></a> Preprint
</h4>

			

<h4>
	<a name='2023'></a> 2023
</h4>
                          <div class="paper short">
				<div class="sub-left">
					<span></span>
					<img src="assets/images/DOP.png" width="240" height="130">
				</div>
				<div class="sub-right">
				<div class="media">
				  <div class="media-body">
					<p class="media-heading">
					  <strong>Disentangling Orthogonal Planes for Indoor Panoramic Room Layout Estimation with Cross-Scale Distortion Awareness</strong><br />
					   <strong>Zhijie Shen</strong>, Zishuo Zheng, Chunyu Lin, Kang Liao, Lang Nie, Shuai Zheng, Yao Zhao<br />
						IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023<br />
					   <a href="">[arXiv]</a>
					   <a href="https://github.com/zhijieshen-bjtu/CVPR2023-DOP-for-Indoor-Panoramic-Room-Layout-Estimation-with-Cross-Scale-Distortion-Awareness">[Github]</a>
					</p>
				  </div>
				</div>	
				</div>
			</div>
				<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img src="assets/images/pano_seg.png" width="240" height="130">
				</div>
				<div class="sub-right">
				<div class="media">
				  <div class="media-body">
					<p class="media-heading">
					  <strong>Complementary Bi-directional Feature Compression for Indoor 360° Semantic Segmentation with Self-distillation</strong><br />
					   Zishuo Zheng, Chunyu Lin, Lang Nie, Kang Liao, <strong>Zhijie Shen</strong>, Yao Zhao<br />
						Winter Conference on Applications of Computer Vision (<strong>WACV</strong>) 2023<br />
					   <a href="https://arxiv.org/abs/2207.02437">[arXiv]</a>
					   <a href="">[Github]</a>
					</p>
				  </div>
				</div>	
				</div>
			</div>
<h4>
	<a name='2022'></a> 2022
</h4>
				<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img src="assets/images/TCSVT22-contourlet.png" width="240" height="130">
				</div>
				<div class="sub-right">
				<div class="media">
				  <div class="media-body">
					<p class="media-heading">
					  <strong>Neural Contourlet Network for Monocular 360° Depth Estimation</strong><br />
					   <strong>Zhijie Shen</strong>, Chunyu Lin, Lang Nie, Kang Liao, Yao Zhao<br />
					   IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>)<br />
					   <a href="https://ieeexplore.ieee.org/document/9833523">[PDF]</a>
					   <a href="https://github.com/zhijieshen-bjtu/Neural-Contourlet-Network-for-MODE">[Github]</a>
					</p>
				  </div>
				</div>	
				</div>
			</div>
			<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img src="assets/images/panoformer.png" width="240" height="130">
				</div>
				<div class="sub-right">
				<div class="media">
				  <div class="media-body">
					<p class="media-heading">
					  <strong>PanoFormer: Panorama Transformer for Indoor 360° Depth Estimation</strong><br />
					   <strong>Zhijie Shen</strong>, Chunyu Lin, Kang Liao, Lang Nie, Zishuo Zheng, Yao Zhao<br />
						European Conference on Computer Vision (<strong>ECCV</strong>), 2022<br />
					   <a href="https://arxiv.org/pdf/2203.09283.pdf">[arXiv]</a>
					   <a href="https://github.com/zhijieshen-bjtu/PanoFormer">[Github]</a>
					</p>
				  </div>
				</div>	
				</div>
			</div>

<h4>
	<a name='2021'></a> 2021
</h4>

		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img src="assets/images/ICME21-dualcube.png" width="240" height="130">
				</div>
				<div class="sub-right">
				<div class="media">
				  <div class="media-body">
					<p class="media-heading">
					  <strong>Distortion-tolerant monocular depth estimation on omnidirectional images using dual-cubemap</strong><br />
					  <strong>Zhijie Shen</strong>; Chunyu Lin; Lang Nie; Kang Liao; Yao Zhao<br />
					   International Conference on Multimedia and Expo (<strong>ICME</strong>), 2021<br />
					   <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9428385">[PDF]</a>
					</p>
				  </div>
				</div>	
				</div>
			</div>
